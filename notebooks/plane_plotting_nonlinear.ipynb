{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import tabulate\n",
    "from sklearn.decomposition import PCA\n",
    "import os \n",
    "\n",
    "# os.chdir('/gpfs/commons/home/tchen/loss_sub_space_geometry_project/loss-subspace-geometry/src')\n",
    "os.chdir('/home/tristan/loss-subspace-geometry-project/loss-subspace-geometry/src')\n",
    "\n",
    "\n",
    "from models.mlp import SubspaceNN, NN, NonLinearSubspaceNN\n",
    "from models.subspace_layers import LinesNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mlp.linear.weight\n",
      "mlp.linear.bias\n",
      "mlp.linear.line.parameterization_linear_1.weight\n",
      "mlp.linear.line.parameterization_linear_1.bias\n",
      "mlp.linear.line.parameterization_linear_2.weight\n",
      "mlp.linear.line.parameterization_linear_2.bias\n",
      "out.weight\n",
      "out.bias\n",
      "out.line.parameterization_linear_1.weight\n",
      "out.line.parameterization_linear_1.bias\n",
      "out.line.parameterization_linear_2.weight\n",
      "out.line.parameterization_linear_2.bias\n",
      "\n",
      "mlp.linear.weight\n",
      "mlp.linear.bias\n",
      "mlp.linear.line.parameterization_linear_1.weight\n",
      "mlp.linear.line.parameterization_linear_1.bias\n",
      "mlp.linear.line.parameterization_linear_2.weight\n",
      "mlp.linear.line.parameterization_linear_2.bias\n",
      "out.weight\n",
      "out.bias\n",
      "out.line.parameterization_linear_1.weight\n",
      "out.line.parameterization_linear_1.bias\n",
      "out.line.parameterization_linear_2.weight\n",
      "out.line.parameterization_linear_2.bias\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# configs\n",
    "data_dim = 784\n",
    "hidden_size = 512\n",
    "out_dim = 10\n",
    "dropout_prob = 0.3\n",
    "seed = 11202022\n",
    "# device = torch.device('cuda')\n",
    "device = torch.device('cuda:2')\n",
    "\n",
    "# model_path = '/gpfs/commons/home/tchen/loss_sub_space_geometry_project/loss-subspace-geometry-save/models/subspace_vanilla_mlp_0.pt'\n",
    "model_path = '/home/tristan/loss-subspace-geometry-project/loss-subspace-geometry-save/models/nonlinear_subspace_vanilla_mlp_epochs_50_2.pt'\n",
    "\n",
    "curve_model = NonLinearSubspaceNN(input_dim=data_dim, \n",
    "                         hidden_dim=hidden_size, \n",
    "                         out_dim=out_dim, \n",
    "                         dropout_prob=dropout_prob, \n",
    "                         seed=seed).to(device)\n",
    "\n",
    "for tuple in curve_model.state_dict():\n",
    "    print(tuple)\n",
    "print()\n",
    "checkpoint = torch.load(model_path)\n",
    "for tuple in checkpoint:\n",
    "    print(tuple)\n",
    "curve_model.load_state_dict(checkpoint, strict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# more configs\n",
    "curve_points = 61\n",
    "grid_points = 21\n",
    "margin_left = 0.2\n",
    "margin_right = 0.2\n",
    "margin_bottom = 0.2\n",
    "margin_top = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NonLinearSubspaceNN(\n",
       "  (mlp): NonLinearSubspaceMLP(\n",
       "    (linear): LinesNN(\n",
       "      in_features=784, out_features=512, bias=True\n",
       "      (line): ParameterizedSubspace(\n",
       "        (parameterization_linear_1): Linear(in_features=1, out_features=10, bias=True)\n",
       "        (parameterization_linear_2): Linear(in_features=10, out_features=401408, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (dropout): Dropout(p=0.3, inplace=False)\n",
       "  )\n",
       "  (out): LinesNN(\n",
       "    in_features=512, out_features=10, bias=True\n",
       "    (line): ParameterizedSubspace(\n",
       "      (parameterization_linear_1): Linear(in_features=1, out_features=10, bias=True)\n",
       "      (parameterization_linear_2): Linear(in_features=10, out_features=5120, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "curve_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "curve_parameters = list(curve_model.parameters())\n",
    "w = []\n",
    "\n",
    "# actual neural net\n",
    "w.append(np.concatenate([\n",
    "        # weights layer 1, biases layer 1, weights layer 2, biases layer 2\n",
    "        p.data.cpu().numpy().ravel() for p in [curve_parameters[0], curve_parameters[1], curve_parameters[6], curve_parameters[7]]\n",
    "    ]))\n",
    "\n",
    "# subspace network 1 parameters\n",
    "w.append(np.concatenate([\n",
    "        # weights layer 1, biases layer 1, weights layer 2, biases layer 2\n",
    "        p.data.cpu().numpy().ravel() for p in [curve_parameters[2], curve_parameters[3], curve_parameters[4], curve_parameters[5]]\n",
    "    ]))\n",
    "\n",
    "# subspace network 2 parameters\n",
    "w.append(np.concatenate([\n",
    "        # weights layer 1, biases layer 1, weights layer 2, biases layer 2\n",
    "        p.data.cpu().numpy().ravel() for p in [curve_parameters[8], curve_parameters[9], curve_parameters[10], curve_parameters[11]]\n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isolated_model = NN(input_dim=data_dim, \n",
    "                         hidden_dim=hidden_size, \n",
    "                         out_dim=out_dim, \n",
    "                         dropout_prob=dropout_prob).to(device)\n",
    "# isolated_checkpoint = torch.load('/gpfs/commons/home/tchen/loss_sub_space_geometry_project/loss-subspace-geometry-save/models/vanilla_mlp.pt')\n",
    "isolated_checkpoint = torch.load('/home/tristan/loss-subspace-geometry-project/loss-subspace-geometry-save/models/vanilla_mlp_0.pt')\n",
    "isolated_model.load_state_dict(isolated_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NN(\n",
       "  (mlp): MLP(\n",
       "    (linear): Linear(in_features=784, out_features=512, bias=True)\n",
       "    (dropout): Dropout(p=0.3, inplace=False)\n",
       "  )\n",
       "  (out): Linear(in_features=512, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isolated_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w.append(np.concatenate([\n",
    "#         p.data.cpu().numpy().ravel() for p in list(isolated_model.parameters())\n",
    "#     ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_subspace(alpha, subspace_net_1, subspace_net_2):\n",
    "    alpha = torch.tensor([float(alpha)])\n",
    "    setattr(subspace_net_1, 'alpha', alpha)\n",
    "    setattr(subspace_net_2, 'alpha', alpha)\n",
    "    w1 = subspace_net_1.get_weight().clone().detach()\n",
    "    b1 = curve_parameters[1].clone().detach().cpu()\n",
    "    w2 = subspace_net_2.get_weight().clone().detach()\n",
    "    b2 = curve_parameters[7].clone().detach().cpu()\n",
    "    weights = torch.cat([w1,b1,w2,b2]).numpy()\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sample uniformly from subspace ##\n",
    "samples = 100\n",
    "alphas = np.random.uniform(low=0, high=1, size=(samples))\n",
    "\n",
    "# instantiate both subspace networks\n",
    "subspace_net_1 = LinesNN(\n",
    "    in_features=data_dim,\n",
    "    out_features=hidden_size\n",
    ")\n",
    "\n",
    "subspace_net_2 = LinesNN(\n",
    "    in_features=hidden_size,\n",
    "    out_features=out_dim\n",
    ")\n",
    "\n",
    "# fetch weights from trained subspace networks\n",
    "subspace_net_1_weights = [curve_parameters[2], curve_parameters[3], curve_parameters[4], curve_parameters[5]]\n",
    "subspace_net_2_weights = [curve_parameters[8], curve_parameters[9], curve_parameters[10], curve_parameters[11]]\n",
    "\n",
    "# set subspace network weights\n",
    "with torch.no_grad():\n",
    "    subspace_net_1.line.parameterization_linear_1.weight.copy_(curve_parameters[2])\n",
    "    subspace_net_1.line.parameterization_linear_1.bias.copy_(curve_parameters[3])\n",
    "    subspace_net_1.line.parameterization_linear_2.weight.copy_(curve_parameters[4])\n",
    "    subspace_net_1.line.parameterization_linear_2.bias.copy_(curve_parameters[5])\n",
    "\n",
    "    subspace_net_2.line.parameterization_linear_1.weight.copy_(curve_parameters[8])\n",
    "    subspace_net_2.line.parameterization_linear_1.bias.copy_(curve_parameters[9])\n",
    "    subspace_net_2.line.parameterization_linear_2.weight.copy_(curve_parameters[10])\n",
    "    subspace_net_2.line.parameterization_linear_2.bias.copy_(curve_parameters[11])\n",
    "\n",
    "# sample points in weight space\n",
    "weight_space_points = []\n",
    "for alpha in alphas:\n",
    "    weights = sample_subspace(alpha, subspace_net_1, subspace_net_2)\n",
    "    weight_space_points.append(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform PCA on sampled weight space points to get top two principle eigenvectors\n",
    "weight_space_points = np.asarray(weight_space_points)\n",
    "pca = PCA().fit(weight_space_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate corners of hyperplane via projection onto principal vecs\n",
    "origin = sample_subspace(0.0, subspace_net_1, subspace_net_2)\n",
    "c3 = sample_subspace(1.0, subspace_net_1, subspace_net_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight space dimensionality: 407050\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# set up for grid for plane plotting\n",
    "\n",
    "def get_xy(point, origin, vector_x, vector_y):\n",
    "    return np.array([np.dot(point - origin, vector_x), np.dot(point - origin, vector_y)])\n",
    "\n",
    "\n",
    "print('Weight space dimensionality: %d' % w[0].shape[0])\n",
    "\n",
    "u = pca.components_[0]\n",
    "dx = np.linalg.norm(u)\n",
    "u /= dx\n",
    "\n",
    "v = pca.components_[-1]\n",
    "# v -= np.dot(u, v) * u\n",
    "dy = np.linalg.norm(v)\n",
    "v /= dy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "c2 = origin + np.dot(u, c3)*u\n",
    "c4 = origin + np.dot(v, c3)*v\n",
    "\n",
    "# w_corners = [origin, c2, c3, c4]\n",
    "w_corners = [origin, c2, c4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0000000e+00 0.0000000e+00]\n",
      " [7.4597794e+01 1.7158514e-05]\n",
      " [1.7407919e-07 6.0337847e-01]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tristan/envs/loss-subspace/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3445: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  if await self.run_code(code, result, async_=asy):\n"
     ]
    }
   ],
   "source": [
    "bend_coordinates = np.stack(get_xy(p, origin, u, v) for p in w_corners)\n",
    "print(bend_coordinates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weights(model: nn.Module, t):\n",
    "    weights = []\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Linear) and 'parameterization' not in name:\n",
    "            # add attribute for weight dimensionality and subspace dimensionality\n",
    "            setattr(module, f'alpha', torch.tensor([t], dtype=torch.float32, device=device))\n",
    "            print(module.get_weight())\n",
    "            weights.extend([module.get_weight(), module.bias.data])\n",
    "        # weights.extend([w for w in module.compute_weights_t(coeffs_t) if w is not None])\n",
    "    return np.concatenate([w.detach().cpu().numpy().ravel() for w in weights])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = np.linspace(0.0, 1.0, curve_points)\n",
    "curve_coordinates = []\n",
    "for t in np.linspace(0.0, 1.0, curve_points):\n",
    "    weights = sample_subspace(t, subspace_net_1, subspace_net_2)\n",
    "    curve_coordinates.append(get_xy(weights, origin, u, v))\n",
    "\n",
    "curve_coordinates = np.stack(curve_coordinates)\n",
    "\n",
    "G = grid_points\n",
    "alphas = np.linspace(0.0 - margin_left, 1.0 + margin_right, G)\n",
    "betas = np.linspace(0.0 - margin_bottom, 1.0 + margin_top, G)\n",
    "\n",
    "tr_loss = np.zeros((G, G))\n",
    "tr_nll = np.zeros((G, G))\n",
    "tr_acc = np.zeros((G, G))\n",
    "tr_err = np.zeros((G, G))\n",
    "\n",
    "te_loss = np.zeros((G, G))\n",
    "te_nll = np.zeros((G, G))\n",
    "te_acc = np.zeros((G, G))\n",
    "te_err = np.zeros((G, G))\n",
    "\n",
    "grid = np.zeros((G, G, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# even more configs for evaluating on FashionMNIST\n",
    "\n",
    "# data_dir = '/gpfs/commons/home/tchen/loss_sub_space_geometry_project/data/'\n",
    "data_dir = '/home/tristan/loss-subspace-geometry-project/data/'\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "FashionMNIST_data_train = torchvision.datasets.FashionMNIST(\n",
    "    data_dir, train=True, transform=transform, download=False)\n",
    "\n",
    "train_set, val_set = torch.utils.data.random_split(\n",
    "    FashionMNIST_data_train, [50000, 10000])\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_set, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(\n",
    "    val_set, batch_size=len(val_set), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(reduction='sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model: nn.Module, loader):\n",
    "    running_loss = 0.0\n",
    "    num_right = 0\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    for i, (x, y) in enumerate(loader):\n",
    "            reshaped_x = x.reshape(x.size(0), 784)\n",
    "            y_hat = model(reshaped_x.to(device))\n",
    "            num_right += torch.sum(\n",
    "                y.to(device) == torch.argmax(\n",
    "                    y_hat, dim=-1)).detach().cpu().item()\n",
    "\n",
    "            running_loss += criterion(y_hat, y.to(device)).item()\n",
    "    return {\n",
    "        'nll': running_loss / len(loader.dataset),\n",
    "        'loss': running_loss / len(loader.dataset),\n",
    "        'accuracy': num_right * 100.0 / len(loader.dataset),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN(\n",
      "  (mlp): MLP(\n",
      "    (linear): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (dropout): Dropout(p=0.3, inplace=False)\n",
      "  )\n",
      "  (out): Linear(in_features=512, out_features=10, bias=True)\n",
      ")\n",
      "----------  ----------  ------------  -----------  -----------------  ----------  ----------------\n",
      "         X           Y    Train loss    Train nll    Train error (%)    Test nll    Test error (%)\n",
      "----------  ----------  ------------  -----------  -----------------  ----------  ----------------\n",
      "   -3.0000     -3.0000        0.3938       0.3938            13.5780      0.3916           13.2300\n",
      "   -3.0000     -1.9500        0.3940       0.3940            13.6020      0.3917           13.2900\n",
      "   -3.0000     -0.9000        0.3942       0.3942            13.5960      0.3919           13.3100\n",
      "   -3.0000      0.1500        0.3945       0.3945            13.6180      0.3922           13.3200\n",
      "   -3.0000      1.2000        0.3948       0.3948            13.6520      0.3925           13.3500\n",
      "   -3.0000      2.2500        0.3951       0.3951            13.6640      0.3929           13.4000\n",
      "   -3.0000      3.3000        0.3956       0.3956            13.6620      0.3933           13.4300\n",
      "   -3.0000      4.3500        0.3960       0.3960            13.7100      0.3937           13.4500\n",
      "   -3.0000      5.4000        0.3964       0.3964            13.7220      0.3941           13.4900\n",
      "   -3.0000      6.4500        0.3969       0.3969            13.7380      0.3946           13.5100\n",
      "   -3.0000      7.5000        0.3974       0.3974            13.7540      0.3950           13.5600\n",
      "   -3.0000      8.5500        0.3978       0.3978            13.7780      0.3955           13.5200\n",
      "   -3.0000      9.6000        0.3983       0.3983            13.8180      0.3959           13.5900\n",
      "   -3.0000     10.6500        0.3988       0.3988            13.8360      0.3964           13.6200\n",
      "   -3.0000     11.7000        0.3992       0.3992            13.8580      0.3968           13.6600\n",
      "   -3.0000     12.7500        0.3997       0.3997            13.8620      0.3973           13.7300\n",
      "   -3.0000     13.8000        0.4001       0.4001            13.8820      0.3977           13.8100\n",
      "   -3.0000     14.8500        0.4006       0.4006            13.8940      0.3982           13.8200\n",
      "   -3.0000     15.9000        0.4010       0.4010            13.8880      0.3986           13.8500\n",
      "   -3.0000     16.9500        0.4015       0.4015            13.9240      0.3991           13.8600\n",
      "   -3.0000     18.0000        0.4020       0.4020            13.9200      0.3995           13.8800\n",
      "----------  ----------  ------------  -----------  -----------------  ----------  ----------------\n",
      "         X           Y    Train loss    Train nll    Train error (%)    Test nll    Test error (%)\n",
      "----------  ----------  ------------  -----------  -----------------  ----------  ----------------\n",
      "   -1.9500     -3.0000        0.3938       0.3938            13.5780      0.3916           13.2100\n",
      "   -1.9500     -1.9500        0.3940       0.3940            13.6000      0.3917           13.2800\n"
     ]
    }
   ],
   "source": [
    "dx_scale = 15\n",
    "dy_scale = 15\n",
    "base_model =  NN(input_dim=data_dim, \n",
    "                         hidden_dim=hidden_size, \n",
    "                         out_dim=out_dim, \n",
    "                         dropout_prob=dropout_prob).to(device)\n",
    "print(base_model)\n",
    "columns = ['X', 'Y', 'Train loss', 'Train nll', 'Train error (%)', 'Test nll', 'Test error (%)']\n",
    "\n",
    "\n",
    "for i, alpha in enumerate(alphas):\n",
    "    for j, beta in enumerate(betas):\n",
    "        p = np.array(origin + alpha * dx_scale*dx * u + beta * dy_scale*dy * v)\n",
    "        offset = 0\n",
    "        for parameter in base_model.parameters():\n",
    "            size = np.prod(parameter.size())\n",
    "            value = p[offset:offset+size].reshape(parameter.size())\n",
    "            parameter.data.copy_(torch.from_numpy(value)).to(device)\n",
    "            offset += size\n",
    "\n",
    "        # tr_res = utils.test(loaders['train'], base_model, criterion, regularizer)\n",
    "        # te_res = utils.test(loaders['test'], base_model, criterion, regularizer)\n",
    "        tr_res = eval(model=base_model, loader=train_loader)\n",
    "        te_res = eval(model=base_model, loader=valid_loader)\n",
    "\n",
    "\n",
    "        tr_loss_v, tr_nll_v, tr_acc_v = tr_res['loss'], tr_res['nll'], tr_res['accuracy']\n",
    "        te_loss_v, te_nll_v, te_acc_v = te_res['loss'], te_res['nll'], te_res['accuracy']\n",
    "\n",
    "        c = get_xy(p, origin, u, v)\n",
    "        grid[i, j] = [alpha * dx_scale * dx, beta * dx_scale * dy]\n",
    "\n",
    "        tr_loss[i, j] = tr_loss_v\n",
    "        tr_nll[i, j] = tr_nll_v\n",
    "        tr_acc[i, j] = tr_acc_v\n",
    "        tr_err[i, j] = 100.0 - tr_acc[i, j]\n",
    "\n",
    "        te_loss[i, j] = te_loss_v\n",
    "        te_nll[i, j] = te_nll_v\n",
    "        te_acc[i, j] = te_acc_v\n",
    "        te_err[i, j] = 100.0 - te_acc[i, j]\n",
    "\n",
    "        values = [\n",
    "            grid[i, j, 0], grid[i, j, 1], tr_loss[i, j], tr_nll[i, j], tr_err[i, j],\n",
    "            te_nll[i, j], te_err[i, j]\n",
    "        ]\n",
    "        table = tabulate.tabulate([values], columns, tablefmt='simple', floatfmt='10.4f')\n",
    "        if j == 0:\n",
    "            table = table.split('\\n')\n",
    "            table = '\\n'.join([table[1]] + table)\n",
    "        else:\n",
    "            table = table.split('\\n')[2]\n",
    "        print(table)\n",
    "\n",
    "np.savez(\n",
    "    os.path.join('./', f'plane_nonlinear_subspace_top_evector_bottom_evector_dxscale{dx_scale}_dyscale{dy_scale}.npz'),\n",
    "    ts=ts,\n",
    "    bend_coordinates=bend_coordinates,\n",
    "    curve_coordinates=curve_coordinates,\n",
    "    alphas=alphas,\n",
    "    betas=betas,\n",
    "    grid=grid,\n",
    "    tr_loss=tr_loss,\n",
    "    tr_acc=tr_acc,\n",
    "    tr_nll=tr_nll,\n",
    "    tr_err=tr_err,\n",
    "    te_loss=te_loss,\n",
    "    te_acc=te_acc,\n",
    "    te_nll=te_nll,\n",
    "    te_err=te_err\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
