{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import tabulate\n",
    "from sklearn.decomposition import PCA\n",
    "import os \n",
    "\n",
    "os.chdir('/home/tristan/loss-subspace-geometry-project/loss-subspace-geometry/src')\n",
    "\n",
    "\n",
    "from models.mlp import SubspaceNN, NN, NonLinearSubspaceNN\n",
    "from models.subspace_layers import LinesNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mlp.linear.weight\n",
      "mlp.linear.bias\n",
      "mlp.linear.line.parameterization_linear_1.weight\n",
      "mlp.linear.line.parameterization_linear_1.bias\n",
      "mlp.linear.line.parameterization_linear_2.weight\n",
      "mlp.linear.line.parameterization_linear_2.bias\n",
      "mlp.linear.line.parameterization_linear_3.weight\n",
      "mlp.linear.line.parameterization_linear_3.bias\n",
      "mlp.linear.line.parameterization_linear_4.weight\n",
      "mlp.linear.line.parameterization_linear_4.bias\n",
      "out.weight\n",
      "out.bias\n",
      "out.line.parameterization_linear_1.weight\n",
      "out.line.parameterization_linear_1.bias\n",
      "out.line.parameterization_linear_2.weight\n",
      "out.line.parameterization_linear_2.bias\n",
      "out.line.parameterization_linear_3.weight\n",
      "out.line.parameterization_linear_3.bias\n",
      "out.line.parameterization_linear_4.weight\n",
      "out.line.parameterization_linear_4.bias\n",
      "mlp.linear.weight\n",
      "mlp.linear.bias\n",
      "mlp.linear.line.parameterization_linear_1.weight\n",
      "mlp.linear.line.parameterization_linear_1.bias\n",
      "mlp.linear.line.parameterization_linear_2.weight\n",
      "mlp.linear.line.parameterization_linear_2.bias\n",
      "mlp.linear.line.parameterization_linear_3.weight\n",
      "mlp.linear.line.parameterization_linear_3.bias\n",
      "mlp.linear.line.parameterization_linear_4.weight\n",
      "mlp.linear.line.parameterization_linear_4.bias\n",
      "out.weight\n",
      "out.bias\n",
      "out.line.parameterization_linear_1.weight\n",
      "out.line.parameterization_linear_1.bias\n",
      "out.line.parameterization_linear_2.weight\n",
      "out.line.parameterization_linear_2.bias\n",
      "out.line.parameterization_linear_3.weight\n",
      "out.line.parameterization_linear_3.bias\n",
      "out.line.parameterization_linear_4.weight\n",
      "out.line.parameterization_linear_4.bias\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# configs\n",
    "data_dim = 784\n",
    "hidden_size = 512\n",
    "out_dim = 10\n",
    "dropout_prob = 0.3\n",
    "seed = 1256\n",
    "train_beta = 0.5\n",
    "vanilla_seed = 450\n",
    "# device = torch.device('cuda')\n",
    "device = torch.device('cuda:2')\n",
    "activation = 'tanh'\n",
    "\n",
    "model_path = f'/home/tristan/loss-subspace-geometry-project/loss-subspace-geometry-save/models/nonlinear_subspace_vanilla_mlp_seed_{seed}_beta_{train_beta}_{activation}_0.pt'\n",
    "\n",
    "curve_model = NonLinearSubspaceNN(input_dim=data_dim, \n",
    "                         hidden_dim=hidden_size, \n",
    "                         out_dim=out_dim, \n",
    "                         dropout_prob=dropout_prob, \n",
    "                         seed=seed).to(device)\n",
    "\n",
    "for tuple in curve_model.state_dict():\n",
    "    print(tuple)\n",
    "\n",
    "checkpoint = torch.load(model_path)\n",
    "for tuple in checkpoint:\n",
    "    print(tuple)\n",
    "curve_model.load_state_dict(checkpoint, strict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# more configs\n",
    "\n",
    "grid_points = 15\n",
    "margin_left = 0.2\n",
    "margin_right = 0.2\n",
    "margin_bottom = 0.2\n",
    "margin_top = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NonLinearSubspaceNN(\n",
       "  (mlp): NonLinearSubspaceMLP(\n",
       "    (linear): LinesNN(\n",
       "      in_features=784, out_features=512, bias=True\n",
       "      (line): ParameterizedSubspace(\n",
       "        (parameterization_linear_1): Linear(in_features=1, out_features=10, bias=True)\n",
       "        (parameterization_linear_2): Linear(in_features=10, out_features=20, bias=True)\n",
       "        (parameterization_linear_3): Linear(in_features=20, out_features=40, bias=True)\n",
       "        (parameterization_linear_4): Linear(in_features=40, out_features=401408, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (dropout): Dropout(p=0.3, inplace=False)\n",
       "  )\n",
       "  (out): LinesNN(\n",
       "    in_features=512, out_features=10, bias=True\n",
       "    (line): ParameterizedSubspace(\n",
       "      (parameterization_linear_1): Linear(in_features=1, out_features=10, bias=True)\n",
       "      (parameterization_linear_2): Linear(in_features=10, out_features=20, bias=True)\n",
       "      (parameterization_linear_3): Linear(in_features=20, out_features=40, bias=True)\n",
       "      (parameterization_linear_4): Linear(in_features=40, out_features=5120, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "curve_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if activation == 'relu':\n",
    "    curve_parameters = list(curve_model.parameters())\n",
    "    for (i,(name, param)) in enumerate(curve_model.named_parameters()):\n",
    "        print(f\"{i}: {name}:\")\n",
    "    w = []\n",
    "\n",
    "    # actual neural net\n",
    "    w.append(np.concatenate([\n",
    "            # weights layer 1, biases layer 1, weights layer 2, biases layer 2\n",
    "            p.data.cpu().numpy().ravel() for p in [curve_parameters[0], curve_parameters[1], curve_parameters[6], curve_parameters[7]]\n",
    "        ]))\n",
    "\n",
    "    # subspace network 1 parameters\n",
    "    w.append(np.concatenate([\n",
    "            # weights layer 1, biases layer 1, weights layer 2, biases layer 2\n",
    "            p.data.cpu().numpy().ravel() for p in [curve_parameters[2], curve_parameters[3], curve_parameters[4], curve_parameters[5]]\n",
    "        ]))\n",
    "\n",
    "    # subspace network 2 parameters\n",
    "    w.append(np.concatenate([\n",
    "            # weights layer 1, biases layer 1, weights layer 2, biases layer 2\n",
    "            p.data.cpu().numpy().ravel() for p in [curve_parameters[8], curve_parameters[9], curve_parameters[10], curve_parameters[11]]\n",
    "    ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: mlp.linear.weight:\n",
      "1: mlp.linear.bias:\n",
      "2: mlp.linear.line.parameterization_linear_1.weight:\n",
      "3: mlp.linear.line.parameterization_linear_1.bias:\n",
      "4: mlp.linear.line.parameterization_linear_2.weight:\n",
      "5: mlp.linear.line.parameterization_linear_2.bias:\n",
      "6: mlp.linear.line.parameterization_linear_3.weight:\n",
      "7: mlp.linear.line.parameterization_linear_3.bias:\n",
      "8: mlp.linear.line.parameterization_linear_4.weight:\n",
      "9: mlp.linear.line.parameterization_linear_4.bias:\n",
      "10: out.weight:\n",
      "11: out.bias:\n",
      "12: out.line.parameterization_linear_1.weight:\n",
      "13: out.line.parameterization_linear_1.bias:\n",
      "14: out.line.parameterization_linear_2.weight:\n",
      "15: out.line.parameterization_linear_2.bias:\n",
      "16: out.line.parameterization_linear_3.weight:\n",
      "17: out.line.parameterization_linear_3.bias:\n",
      "18: out.line.parameterization_linear_4.weight:\n",
      "19: out.line.parameterization_linear_4.bias:\n"
     ]
    }
   ],
   "source": [
    "if activation == 'tanh':\n",
    "    curve_parameters = list(curve_model.parameters())\n",
    "    for (i,(name, param)) in enumerate(curve_model.named_parameters()):\n",
    "        print(f\"{i}: {name}:\")\n",
    "    w = []\n",
    "\n",
    "    # actual neural net\n",
    "    w.append(np.concatenate([\n",
    "            # weights layer 1, biases layer 1, weights layer 2, biases layer 2\n",
    "            p.data.cpu().numpy().ravel() for p in [curve_parameters[0], curve_parameters[1], curve_parameters[10], curve_parameters[11]]\n",
    "        ]))\n",
    "\n",
    "    # subspace network 1 parameters\n",
    "    w.append(np.concatenate([\n",
    "            # weights layer 1, biases layer 1, weights layer 2, biases layer 2\n",
    "            p.data.cpu().numpy().ravel() for p in curve_parameters[2:10]\n",
    "        ]))\n",
    "\n",
    "    # subspace network 2 parameters\n",
    "    w.append(np.concatenate([\n",
    "            # weights layer 1, biases layer 1, weights layer 2, biases layer 2\n",
    "            p.data.cpu().numpy().ravel() for p in curve_parameters[12:20]\n",
    "    ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isolated_model = NN(input_dim=data_dim, \n",
    "                         hidden_dim=hidden_size, \n",
    "                         out_dim=out_dim, \n",
    "                         dropout_prob=dropout_prob).to(device)\n",
    "isolated_checkpoint = torch.load(f'/home/tristan/loss-subspace-geometry-project/loss-subspace-geometry-save/models/vanilla_mlp_seed_{vanilla_seed}_0.pt')\n",
    "isolated_model.load_state_dict(isolated_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NN(\n",
       "  (mlp): MLP(\n",
       "    (linear): Linear(in_features=784, out_features=512, bias=True)\n",
       "    (dropout): Dropout(p=0.3, inplace=False)\n",
       "  )\n",
       "  (out): Linear(in_features=512, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isolated_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "isolated_weights = np.concatenate([\n",
    "        p.data.cpu().numpy().ravel() for p in list(isolated_model.parameters())\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_subspace_relu(alpha, subspace_net_1, subspace_net_2):\n",
    "    alpha = torch.tensor([float(alpha)])\n",
    "    setattr(subspace_net_1, 'alpha', alpha)\n",
    "    setattr(subspace_net_2, 'alpha', alpha)\n",
    "    w1 = subspace_net_1.get_weight().clone().detach()\n",
    "    b1 = curve_parameters[1].clone().detach().cpu()\n",
    "    w2 = subspace_net_2.get_weight().clone().detach()\n",
    "    b2 = curve_parameters[7].clone().detach().cpu()\n",
    "    weights = torch.cat([w1,b1,w2,b2]).numpy()\n",
    "    return weights\n",
    "\n",
    "def sample_subspace_tanh(alpha, subspace_net_1, subspace_net_2):\n",
    "    alpha = torch.tensor([float(alpha)])\n",
    "    setattr(subspace_net_1, 'alpha', alpha)\n",
    "    setattr(subspace_net_2, 'alpha', alpha)\n",
    "    w1 = subspace_net_1.get_weight().clone().detach()\n",
    "    b1 = curve_parameters[1].clone().detach().cpu()\n",
    "    w2 = subspace_net_2.get_weight().clone().detach()\n",
    "    b2 = curve_parameters[11].clone().detach().cpu()\n",
    "    weights = torch.cat([w1,b1,w2,b2]).numpy()\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tristan/envs/loss-subspace/lib/python3.8/site-packages/torch/nn/functional.py:1795: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    }
   ],
   "source": [
    "## Sample uniformly from subspace ##\n",
    "samples = 1000\n",
    "alphas = np.linspace(0, 1, samples)\n",
    "\n",
    "# instantiate both subspace networks\n",
    "subspace_net_1 = LinesNN(\n",
    "    in_features=data_dim,\n",
    "    out_features=hidden_size\n",
    ")\n",
    "\n",
    "subspace_net_2 = LinesNN(\n",
    "    in_features=hidden_size,\n",
    "    out_features=out_dim\n",
    ")\n",
    "\n",
    "if activation == \"tanh\":\n",
    "    # set subspace network weights\n",
    "    sample_subspace = sample_subspace_tanh\n",
    "    with torch.no_grad():\n",
    "        subspace_net_1.line.parameterization_linear_1.weight.copy_(curve_parameters[2])\n",
    "        subspace_net_1.line.parameterization_linear_1.bias.copy_(curve_parameters[3])\n",
    "        subspace_net_1.line.parameterization_linear_2.weight.copy_(curve_parameters[4])\n",
    "        subspace_net_1.line.parameterization_linear_2.bias.copy_(curve_parameters[5])\n",
    "        subspace_net_1.line.parameterization_linear_3.weight.copy_(curve_parameters[6])\n",
    "        subspace_net_1.line.parameterization_linear_3.bias.copy_(curve_parameters[7])\n",
    "        subspace_net_1.line.parameterization_linear_4.weight.copy_(curve_parameters[8])\n",
    "        subspace_net_1.line.parameterization_linear_4.bias.copy_(curve_parameters[9])\n",
    "\n",
    "        subspace_net_2.line.parameterization_linear_1.weight.copy_(curve_parameters[12])\n",
    "        subspace_net_2.line.parameterization_linear_1.bias.copy_(curve_parameters[13])\n",
    "        subspace_net_2.line.parameterization_linear_2.weight.copy_(curve_parameters[14])\n",
    "        subspace_net_2.line.parameterization_linear_2.bias.copy_(curve_parameters[15])\n",
    "        subspace_net_2.line.parameterization_linear_3.weight.copy_(curve_parameters[16])\n",
    "        subspace_net_2.line.parameterization_linear_3.bias.copy_(curve_parameters[17])\n",
    "        subspace_net_2.line.parameterization_linear_4.weight.copy_(curve_parameters[18])\n",
    "        subspace_net_2.line.parameterization_linear_4.bias.copy_(curve_parameters[19])\n",
    "\n",
    "if activation == \"relu\":\n",
    "    sample_subspace = sample_subspace_relu\n",
    "    # set subspace network weights\n",
    "    with torch.no_grad():\n",
    "        subspace_net_1.line.parameterization_linear_1.weight.copy_(curve_parameters[2])\n",
    "        subspace_net_1.line.parameterization_linear_1.bias.copy_(curve_parameters[3])\n",
    "        subspace_net_1.line.parameterization_linear_2.weight.copy_(curve_parameters[4])\n",
    "        subspace_net_1.line.parameterization_linear_2.bias.copy_(curve_parameters[5])\n",
    "\n",
    "        subspace_net_2.line.parameterization_linear_1.weight.copy_(curve_parameters[8])\n",
    "        subspace_net_2.line.parameterization_linear_1.bias.copy_(curve_parameters[9])\n",
    "        subspace_net_2.line.parameterization_linear_2.weight.copy_(curve_parameters[10])\n",
    "        subspace_net_2.line.parameterization_linear_2.bias.copy_(curve_parameters[11])\n",
    "\n",
    "# sample points in weight space\n",
    "weight_space_points = []\n",
    "for alpha in alphas:\n",
    "    weights = sample_subspace(alpha, subspace_net_1, subspace_net_2)\n",
    "    weight_space_points.append(weights)\n",
    "\n",
    "weight_space_points = np.array(weight_space_points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save weight space samples\n",
    "np.savez(f'subspace_samples_seed_{seed}_beta_{train_beta}_{activation}.npz', weight_space_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform PCA on sampled weight space points to get top two principle eigenvectors\n",
    "weight_space_points = np.asarray(weight_space_points)\n",
    "pca = PCA().fit(weight_space_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tristan/envs/loss-subspace/lib/python3.8/site-packages/torch/nn/functional.py:1795: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    }
   ],
   "source": [
    "# calculate corners of hyperplane via projection onto principal vecs\n",
    "origin = sample_subspace(0.0, subspace_net_1, subspace_net_2)\n",
    "c3 = sample_subspace(1.0, subspace_net_1, subspace_net_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight space dimensionality: 407050\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# set up for grid for plane plotting\n",
    "\n",
    "def get_xy(point, origin, vector_x, vector_y):\n",
    "    return np.array([np.dot(point - origin, vector_x), np.dot(point - origin, vector_y)])\n",
    "\n",
    "\n",
    "print('Weight space dimensionality: %d' % w[0].shape[0])\n",
    "\n",
    "u = c3-origin\n",
    "dx = np.linalg.norm(u)\n",
    "u /= dx\n",
    "\n",
    "v = np.random.uniform(size=u.shape)\n",
    "dy = np.linalg.norm(v)\n",
    "v /= dy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_corners = [origin, c3, v]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tristan/envs/loss-subspace/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3445: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  if await self.run_code(code, result, async_=asy):\n"
     ]
    }
   ],
   "source": [
    "bend_coordinates = np.stack(get_xy(p, origin, u, v) for p in w_corners)\n",
    "curve_coordinates = np.stack(get_xy(p, origin, u, v) for p in weight_space_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weights(model: nn.Module, t):\n",
    "    weights = []\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Linear) and 'parameterization' not in name:\n",
    "            # add attribute for weight dimensionality and subspace dimensionality\n",
    "            setattr(module, f'alpha', torch.tensor([t], dtype=torch.float32, device=device))\n",
    "            print(module.get_weight())\n",
    "            weights.extend([module.get_weight(), module.bias.data])\n",
    "        # weights.extend([w for w in module.compute_weights_t(coeffs_t) if w is not None])\n",
    "    return np.concatenate([w.detach().cpu().numpy().ravel() for w in weights])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = grid_points\n",
    "alphas = np.linspace(0.0 - margin_left, 1.0 + margin_right, G)\n",
    "betas = np.linspace(0.0 - margin_bottom, 1.0 + margin_top, G)\n",
    "\n",
    "tr_loss = np.zeros((G, G))\n",
    "tr_nll = np.zeros((G, G))\n",
    "tr_acc = np.zeros((G, G))\n",
    "tr_err = np.zeros((G, G))\n",
    "\n",
    "te_loss = np.zeros((G, G))\n",
    "te_nll = np.zeros((G, G))\n",
    "te_acc = np.zeros((G, G))\n",
    "te_err = np.zeros((G, G))\n",
    "\n",
    "grid = np.zeros((G, G, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# even more configs for evaluating on FashionMNIST\n",
    "data_dir = '/home/tristan/loss-subspace-geometry-project/data/'\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "FashionMNIST_data_train = torchvision.datasets.FashionMNIST(\n",
    "    data_dir, train=True, transform=transform, download=False)\n",
    "\n",
    "train_set, val_set = torch.utils.data.random_split(\n",
    "    FashionMNIST_data_train, [50000, 10000])\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_set, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(\n",
    "    val_set, batch_size=len(val_set), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(reduction='sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model: nn.Module, loader):\n",
    "    running_loss = 0.0\n",
    "    num_right = 0\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    for i, (x, y) in enumerate(loader):\n",
    "            reshaped_x = x.reshape(x.size(0), 784)\n",
    "            y_hat = model(reshaped_x.to(device))\n",
    "            num_right += torch.sum(\n",
    "                y.to(device) == torch.argmax(\n",
    "                    y_hat, dim=-1)).detach().cpu().item()\n",
    "\n",
    "            running_loss += criterion(y_hat, y.to(device)).item()\n",
    "    return {\n",
    "        'nll': running_loss / len(loader.dataset),\n",
    "        'loss': running_loss / len(loader.dataset),\n",
    "        'accuracy': num_right * 100.0 / len(loader.dataset),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.00000000e+00  0.00000000e+00]\n",
      " [ 2.00147241e-01 -1.17203344e-03]\n",
      " [ 4.00422335e-01 -2.34537752e-03]\n",
      " ...\n",
      " [ 2.23019348e+02 -1.34763771e+00]\n",
      " [ 2.23254654e+02 -1.34907504e+00]\n",
      " [ 2.23489960e+02 -1.35051280e+00]]\n",
      "----------  ----------  ------------  -----------  -----------------  ----------  ----------------\n",
      "         X           Y    Train loss    Train nll    Train error (%)    Test nll    Test error (%)\n",
      "----------  ----------  ------------  -----------  -----------------  ----------  ----------------\n",
      " -223.4900   -368.4654        2.4433       2.4433            90.0000      2.4425           90.0000\n",
      " -223.4900   -184.2327        2.3874       2.3874            90.0100      2.3871           89.9500\n",
      " -223.4900     -0.0000        0.2083       0.2083             7.8080      0.2113            7.9800\n",
      " -223.4900    184.2327     1336.0825    1336.0825            90.0360   1324.3607           89.8200\n",
      " -223.4900    368.4654     3673.9278    3673.9278            90.0360   3647.1656           89.8200\n",
      " -223.4900    552.6982     6632.5813    6632.5813            90.0360   6588.7788           89.8200\n",
      " -223.4900    736.9309    10212.0451   10212.0451            90.0360  10149.2088           89.8200\n",
      " -223.4900    921.1636    14412.3172   14412.3172            90.0360  14328.4448           89.8200\n",
      " -223.4900   1105.3963    19233.3978   19233.3978            90.0360  19126.4960           89.8200\n",
      " -223.4900   1289.6290    24675.2915   24675.2915            90.0360  24543.3552           89.8200\n",
      " -223.4900   1473.8617    30737.9963   30737.9963            90.0360  30579.0272           89.8200\n",
      " -223.4900   1658.0945    37421.4932   37421.4932            90.0360  37233.4944           89.8200\n",
      " -223.4900   1842.3272    44725.8019   44725.8019            90.0360  44506.7808           89.8200\n",
      " -223.4900   2026.5599    52650.9161   52650.9161            90.0360  52398.8800           89.8200\n",
      " -223.4900   2210.7926    61196.8646   61196.8646            90.0360  60909.8048           89.8300\n",
      "----------  ----------  ------------  -----------  -----------------  ----------  ----------------\n",
      "         X           Y    Train loss    Train nll    Train error (%)    Test nll    Test error (%)\n",
      "----------  ----------  ------------  -----------  -----------------  ----------  ----------------\n",
      " -111.7450   -368.4654        2.4433       2.4433            90.0000      2.4425           90.0000\n",
      " -111.7450   -184.2327        2.3874       2.3874            90.0100      2.3871           89.9500\n",
      " -111.7450     -0.0000        0.2035       0.2035             7.6060      0.2077            7.7100\n",
      " -111.7450    184.2327     1310.9609    1310.9609            90.0360   1299.3677           89.8200\n",
      " -111.7450    368.4654     3636.4105    3636.4105            90.0360   3609.8664           89.8200\n",
      " -111.7450    552.6982     6582.6689    6582.6689            90.0360   6539.1760           89.8200\n",
      " -111.7450    736.9309    10149.7368   10149.7368            90.0360  10087.2976           89.8200\n",
      " -111.7450    921.1636    14337.6134   14337.6134            90.0360  14254.2288           89.8200\n",
      " -111.7450   1105.3963    19146.2984   19146.2984            90.0360  19039.9696           89.8200\n",
      " -111.7450   1289.6290    24575.7961   24575.7961            90.0360  24444.5280           89.8200\n",
      " -111.7450   1473.8617    30626.1054   30626.1054            90.0360  30467.9008           89.8200\n",
      " -111.7450   1658.0945    37297.2074   37297.2074            90.0360  37110.0704           89.8200\n",
      " -111.7450   1842.3272    44589.1209   44589.1209            90.0360  44371.0464           89.8200\n",
      " -111.7450   2026.5599    52501.8370   52501.8370            90.0360  52250.8160           89.8200\n",
      " -111.7450   2210.7926    61035.3948   61035.3948            90.0360  60749.4592           89.8300\n",
      "----------  ----------  ------------  -----------  -----------------  ----------  ----------------\n",
      "         X           Y    Train loss    Train nll    Train error (%)    Test nll    Test error (%)\n",
      "----------  ----------  ------------  -----------  -----------------  ----------  ----------------\n",
      "   -0.0000   -368.4654        2.4433       2.4433            90.0000      2.4425           90.0000\n",
      "   -0.0000   -184.2327        2.3874       2.3874            90.0100      2.3871           89.9500\n",
      "   -0.0000     -0.0000        0.2008       0.2008             7.4860      0.2059            7.5400\n",
      "   -0.0000    184.2327     1286.0331    1286.0331            90.0360   1274.5674           89.8200\n",
      "   -0.0000    368.4654     3599.0872    3599.0872            90.0360   3572.7600           89.8200\n",
      "   -0.0000    552.6982     6532.9499    6532.9499            90.0360   6489.7640           89.8200\n",
      "   -0.0000    736.9309    10087.6226   10087.6226            90.0360  10025.5816           89.8200\n",
      "   -0.0000    921.1636    14263.1038   14263.1038            90.0360  14180.2064           89.8200\n"
     ]
    }
   ],
   "source": [
    "dxy_scale = 5\n",
    "base_model =  NN(input_dim=data_dim, \n",
    "                         hidden_dim=hidden_size, \n",
    "                         out_dim=out_dim, \n",
    "                         dropout_prob=dropout_prob).to(device)\n",
    "print(curve_coordinates)\n",
    "columns = ['X', 'Y', 'Train loss', 'Train nll', 'Train error (%)', 'Test nll', 'Test error (%)']\n",
    "\n",
    "\n",
    "for i, alpha in enumerate(alphas):\n",
    "    for j, beta in enumerate(betas):\n",
    "        p = np.array(origin + alpha * dxy_scale*dx * u + beta * dxy_scale*dy * v)\n",
    "        offset = 0\n",
    "        for parameter in base_model.parameters():\n",
    "            size = np.prod(parameter.size())\n",
    "            value = p[offset:offset+size].reshape(parameter.size())\n",
    "            parameter.data.copy_(torch.from_numpy(value)).to(device)\n",
    "            offset += size\n",
    "\n",
    "        # tr_res = utils.test(loaders['train'], base_model, criterion, regularizer)\n",
    "        # te_res = utils.test(loaders['test'], base_model, criterion, regularizer)\n",
    "        tr_res = eval(model=base_model, loader=train_loader)\n",
    "        te_res = eval(model=base_model, loader=valid_loader)\n",
    "\n",
    "\n",
    "        tr_loss_v, tr_nll_v, tr_acc_v = tr_res['loss'], tr_res['nll'], tr_res['accuracy']\n",
    "        te_loss_v, te_nll_v, te_acc_v = te_res['loss'], te_res['nll'], te_res['accuracy']\n",
    "\n",
    "        c = get_xy(p, origin, u, v)\n",
    "        grid[i, j] = [alpha * dxy_scale * dx, beta * dxy_scale * dy]\n",
    "\n",
    "        tr_loss[i, j] = tr_loss_v\n",
    "        tr_nll[i, j] = tr_nll_v\n",
    "        tr_acc[i, j] = tr_acc_v\n",
    "        tr_err[i, j] = 100.0 - tr_acc[i, j]\n",
    "\n",
    "        te_loss[i, j] = te_loss_v\n",
    "        te_nll[i, j] = te_nll_v\n",
    "        te_acc[i, j] = te_acc_v\n",
    "        te_err[i, j] = 100.0 - te_acc[i, j]\n",
    "\n",
    "        values = [\n",
    "            grid[i, j, 0], grid[i, j, 1], tr_loss[i, j], tr_nll[i, j], tr_err[i, j],\n",
    "            te_nll[i, j], te_err[i, j]\n",
    "        ]\n",
    "        table = tabulate.tabulate([values], columns, tablefmt='simple', floatfmt='10.4f')\n",
    "        if j == 0:\n",
    "            table = table.split('\\n')\n",
    "            table = '\\n'.join([table[1]] + table)\n",
    "        else:\n",
    "            table = table.split('\\n')[2]\n",
    "        print(table)\n",
    "\n",
    "np.savez(\n",
    "    os.path.join('./', f'plane_nonlinear_subspace_edges_random_dir_dxscale{dxy_scale}_dyscale{dxy_scale}_seed_{seed}_vanillaseed_{vanilla_seed}_beta_{train_beta}_activation_{activation}.npz'),\n",
    "    bend_coordinates=bend_coordinates,\n",
    "    curve_coordinates=curve_coordinates,\n",
    "    alphas=alphas,\n",
    "    betas=betas,\n",
    "    grid=grid,\n",
    "    tr_loss=tr_loss,\n",
    "    tr_acc=tr_acc,\n",
    "    tr_nll=tr_nll,\n",
    "    tr_err=tr_err,\n",
    "    te_loss=te_loss,\n",
    "    te_acc=te_acc,\n",
    "    te_nll=te_nll,\n",
    "    te_err=te_err\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
